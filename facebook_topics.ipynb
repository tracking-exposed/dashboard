{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Topical Model of Facebook's Topics aiming towards Users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "# for word vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import nltk\n",
    "import pprint\n",
    "import re\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from ast import literal_eval # to import the column texts as list\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(s):\n",
    "    try:\n",
    "        return detect(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# split into words\n",
    "def get_words(raw, language):\n",
    "    \"\"\"\n",
    "    :raw: text that has not been cleaned yet\n",
    "    output: Stemmed Tokens\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    tokens = word_tokenize(raw)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if not w in set(stopwords.words(language))]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    return stemmed\n",
    "\n",
    "def get_topic_distribution(lda_model, raw_input, dictionary, language):\n",
    "    \"\"\"Return a vecor of topical distribution of each document or text. \n",
    "    :param lda_model: the output of the function gensim.models.ldamodel.LdaModel\n",
    "    :param raw_imput: raw chinese policy text or doc\n",
    "    :param dictionary: the output of corpora.Dictionary() function which is the vocab.\n",
    "    \"\"\"\n",
    "    from pandas import DataFrame\n",
    "    other_texts = [ # needs tokenized\n",
    "        get_words(raw_input, language)\n",
    "    ]\n",
    "    #dictionary = Dictionary(sentences)\n",
    "    other_corpus = [dictionary.doc2bow(text) for text in other_texts]\n",
    "    unseen_doc = other_corpus[0]\n",
    "    vector = lda_model[unseen_doc][0]\n",
    "    return(DataFrame.from_records(vector)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert the path to the output of dowload.py below (instead of /home/ubuntu..user_a.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df = pd.read_csv('sample_data/users/user_a.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Users\n",
    "dict(df.user.groupby(df.user).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'rosemary-ceviche-meringue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a random sample and see language distribution\n",
    "random.seed(10)\n",
    "len_sample = 1000\n",
    "sample = df.sample(len_sample)[['texts', 'nature', 'publicationTime']]\n",
    "sample.texts = sample.texts.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['concatLanguage'] = sample.texts.apply(''.join).apply(lang_detect)\n",
    "set(sample.concatLanguage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.concatLanguage.groupby([sample.concatLanguage]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.concatLanguage.groupby([sample.concatLanguage]).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset posts with the same language and tockenize words\n",
    "sample = sample[(sample.concatLanguage=='es')]\n",
    "language = 'spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to install ssl certificates for the python version you are using, checkout the web for how to do it.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# concatenate texts break into tokens for each sentence\n",
    "sample['concatenatedText'] = sample.texts.apply(''.join)\n",
    "import time\n",
    "start = time.time()\n",
    "sent = sample.concatenatedText\n",
    "\n",
    "sentences = []\n",
    "for i in sent:\n",
    "    sentences.append(get_words(i, language))\n",
    "end = time.time()\n",
    "print(end-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of topics\n",
    "num_topics = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import gensim\n",
    "import os\n",
    "import dill\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import time\n",
    "\n",
    "# Create Dictionary\n",
    "start = time.time()\n",
    "id2word = corpora.Dictionary(sentences)\n",
    "dictionary = Dictionary(sentences)\n",
    "# Create Corpus\n",
    "texts = sentences\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True,\n",
    "                                           minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)\n",
    "#pyLDAvis.show(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the original dataset with the Topical Distribution spread "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "buckets = []\n",
    "for i in sample.concatenatedText:\n",
    "    buckets.append(get_topic_distribution(lda_model,i,dictionary, language))\n",
    "groups= []\n",
    "for i in range(0,len(buckets)):\n",
    "    groups.append(pd.Series.to_frame((buckets[i])).T)\n",
    "\n",
    "groups = pd.concat(groups).reset_index()\n",
    "DATA =  pd.concat([sample.reset_index(drop=True), groups],axis=1).reset_index(drop=True)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "import dill\n",
    "dill.dump_session('FBenv.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.load_session('FBenv.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns = [           'texts',           'nature',  'publicationTime',\n",
    "         'concatLanguage', 'concatenatedText',            'index',\n",
    "                        'Other','Politics','Advertisement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're interested only in politics, so we sum the other columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIC = DATA[DATA.nature == 'organic']\n",
    "SPONSOR = DATA[DATA.nature == 'sponsored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import pandas as pd\n",
    "import math\n",
    "import math\n",
    "from bubbly.bubbly import add_slider_steps\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "ORGANIC['publicationTime']=pd.to_datetime(ORGANIC['publicationTime']).dt.date\n",
    "EX = pd.DataFrame(ORGANIC.groupby('publicationTime')[['Politics','Advertisement','Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x= EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x= x,\n",
    "    y= EX['Advertisement'],\n",
    "    name='Advertisement',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x= x,\n",
    "    y= EX['Politics'],\n",
    "    name='Politics',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x= x,\n",
    "    y= EX['Other'],\n",
    "    name='Other',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    \n",
    "    legend=dict(x=0, y=-.13,\n",
    "       orientation=\"h\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topical Distribution Spread from Organic Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EX = pd.DataFrame(SPONSOR.groupby('publicationTime')[['Politics','Advertisement','Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x= EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x= x,\n",
    "    y= EX['Advertisement'],\n",
    "    name='Sales Ad',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x= x,\n",
    "    y= EX['Politics'],\n",
    "    name='Political Ad',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x= x,\n",
    "    y= EX['Other'],\n",
    "    name='Other Ad',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    \n",
    "    legend=dict(x=0, y=-.13,\n",
    "       orientation=\"h\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sponsored Posts Topical Spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much does each User's gets exposed to these Political Advertisements? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now ananlyze the User's exposure to these Politcal, Advertisements\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = df[df['user']==user]\n",
    "USER['publicationTime'] = pd.to_datetime(USER['publicationTime']).dt.date\n",
    "USER = USER.sort_values(by='publicationTime')\n",
    "USER['concatenatedText'] = USER.texts.apply(''.join)\n",
    "USER = USER[['concatenatedText', 'nature', 'publicationTime']]\n",
    "sent = USER.concatenatedText\n",
    "sentences = []\n",
    "for i in sent:\n",
    "    sentences.append(get_words(i, language))\n",
    "\n",
    "id2word = corpora.Dictionary(sentences)\n",
    "dictionary = Dictionary(sentences)\n",
    "# Create Corpus\n",
    "texts = sentences\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True,\n",
    "                                           minimum_probability=0.0)\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = []\n",
    "for i in USER.concatenatedText:\n",
    "    buckets.append(get_topic_distribution(lda_model,i,dictionary, language))\n",
    "groups= []\n",
    "for i in range(0,len(buckets)):\n",
    "    groups.append(pd.Series.to_frame((buckets[i])).T)\n",
    "\n",
    "groups = pd.concat(groups).reset_index()\n",
    "USER_DATA =  pd.concat([USER.reset_index(drop=True), groups],axis=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_DATA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_DATA.columns = ['concatenatedText', 'nature', 'publicationTime', 'index', 'Advertisement', 'Other', 'Politics']\n",
    "\n",
    "USER_ORGANIC = USER_DATA[USER_DATA.nature == 'organic']\n",
    "USER_SPONSOR = USER_DATA[USER_DATA.nature == 'sponsored']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does Facebook impact A user's timeline impression during Political Elections?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For $USER  Personal timeline impression exposure (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bubbly.bubbly import add_slider_steps\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "EX = pd.DataFrame(USER_ORGANIC.groupby('publicationTime')[['Politics','Advertisement','Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x= EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x= x,\n",
    "    y= EX['Advertisement'],\n",
    "    name='Organic Forwarded Ads',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x= x,\n",
    "    y= EX['Politics'],\n",
    "    name='Organic Political Posts',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x= x,\n",
    "    y= EX['Other'],\n",
    "    name='Other Organic Posts',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "data_organic = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "\n",
    "    legend=dict(x=0, y=-.13,\n",
    "       orientation=\"h\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data_organic, layout=layout)\n",
    "iplot(fig) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sponsored Posts for \\$User:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EX = pd.DataFrame(USER_SPONSOR.groupby('publicationTime')[['Politics','Advertisement','Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x= EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x= x,\n",
    "    y= EX['Advertisement'],\n",
    "    name='Sponsored Advertisement',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x= x,\n",
    "    y= EX['Politics'],\n",
    "    name='Sponsored Politics',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x= x,\n",
    "    y= EX['Other'],\n",
    "    name='Other Sponsors',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "              #color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "data_sponsored = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    yaxis=dict( autorange=True,\n",
    "               fixedrange = False),\n",
    "    legend=dict(x=0, y=-.13,\n",
    "       orientation=\"h\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data_sponsored, layout=layout)\n",
    "iplot(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
